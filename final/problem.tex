\textcolor[rgb]{0.00,0.00,1.00}{Our goal is to find data-driven functional models for cyber-physical systems starting from available historical data. The aim is to build models that relate the value of the response variables $\mathrm{y}_1,\ldots,\mathrm{y}_p$ with the values of the predictor variables or features $\mathrm{x}_1,\ldots,\mathrm{x}_s$, and that can be used to setup an optimization predictive control problem. For example, in the case of buildings the response variables can be represented by the power consumption or the room temperatures, while the features can be represented by weather data, set-points information and building schedules.}
\textcolor[rgb]{1.00,0.00,0.00}{When the dataset has lots of features, as in the case of large buildings, which interact in complicated nonlinear ways, assembling a single global model, such as linear or polynomial regression, can be difficult and lead to poor response predictions. An approach to non-linear regression is to partition the data space into smaller regions $R_i$, where the interactions are more manageable. Regression trees are an example of an algorithm which belongs to the class of recursive partitioning algorithms. The seminal algorithm for learning regression trees is CART as described in~\cite{BreimanFriedmanStoneEtAl1984}. Regression trees-based approaches are our choice to construct data-driven models for cyber-physical systems. The primary reason for this modeling choice is that regression trees are highly interpretable by design. Interpretability is a fundamental desirable quality in any predictive model. Complex predictive models like neural networks, support vector regression \etc go through a long calculation routine and involve too many factors. It is not easy for a human engineer to judge if the operation/decision is correct or not or how it was generated in the first place. For example, building operators are used to operating a system with fixed logic and rules. They tend to prefer models that are more transparent, where it is clear exactly which factors were used to make a particular prediction. At each node of a regression tree, a simple "if this then that" human readable plain text rule is applied to generate a prediction at the leafs, which anyone can easily understand and interpret. Making machine learning algorithms more interpretable is an active area of research, one that is essential for incorporating human centric models in cyber-physical systems.}

\textcolor[rgb]{0.00,0.00,1.00}{We want to use such regression trees-based models to setup optimal predictive control strategies as in the classical MPC formulation. The main problem rising up when dealing with machine learning algorithms is that they do not provide mathematical models that are directly usable for control. To overcome this issue we propose a solution that is based on the \emph{separation of variables}. In particular, given a dataset $(\mathcal{X},\mathcal{Y})$, where $\mathcal{X}=\{\x_1,\ldots,\x_s\}$ is the set of predictor variables (or features) and $\mathcal{Y}=\{\y_1,\ldots,\y_p\}$ is of response variables, we want to use regression trees to learn a model $f_i$ to predict the response $\y_i$ as }
\textcolor[rgb]{0.00,0.00,1.00}{
\begin{equation}\label{eq:RTmodel}
	\y_i=f_i(\x_1,\ldots,\x_s)
\end{equation}
}

\textcolor[rgb]{0.00,0.00,1.00}{
Given a forecast of predictor variables $\hat{\x}_i$, we can predict system response using \eqref{eq:RTmodel}. However, suppose the set of system inputs $\mathcal{X}_c$, that are the variables we can manipulate is a subset of the features set, i.e. $\mathcal{X}_c=\{\mathrm{u}_1,\ldots,\mathrm{u}_m\}\subset\mathcal{X}$. Suppose we want to setup a receding horizon control problem to find optimal inputs that optimize a performance cost function. Let $u(t)=[\mathrm{u}_1(t),\ldots,\mathrm{u}_m(t)]$ be the vector of inputs applied to the system at time $t$ to be optimized, $x(t) = [\mathrm{x}_1(t),\ldots,\mathrm{x}_{s-m}(t)]$ be the vector of system variables measured at time $t$, and $y(t) = [\mathrm{y}_1(t),\ldots,\mathrm{y}_{p}(t)]$ be the vector of system response variables at time $t$, then model \eqref{eq:RTmodel} can not be used to solve the following optimal predictive control problem}

\textcolor[rgb]{0.00,0.00,1.00}{
\begin{equation}\label{eq:linear_program}
\begin{aligned}
& \underset{u_{t+k}\in\mathcal{X}_c}{\text{minimize}} & &  \sum_{k = 0}^{N}{J(y_{t+k},u_{t+k})}  \\
& \text{subject to }                                  & &  y_{t+k}    =   f(x_{t+k},u_{t+k})     \\
&                                                     & &  u_{t+k}   \in \mathcal{\bar U}        \\
&                                                     & &  y_{t+k}   \in \mathcal{\bar Y}        \\
&                                                     & &  x_{t}      =   x(t)                   \\
&                                                     & &  k          =   0,\ldots,N             \\
\end{aligned}
\end{equation}
This is because inputs $u$ are optimization variables, hence they are not known a priori to be used as features to build the prediction using \eqref{eq:RTmodel}.}

\textcolor[rgb]{0.00,0.00,1.00}{
\emph{Separation of variables} is a technique that consists on partitioning the features dataset into inputs (or control variables) and disturbances sets, i.e. the sets of variables we can and can not manipulate, respectively. More precisely, $\mathcal{X} = \mathcal{X}_c\biguplus\mathcal{X}_d$, where $\biguplus$ represents the disjoint union. The regression tree is trained only using the disturbance variables in $\mathcal{X}_d$, so that we can associate to each leaf a model that depends only on the inputs data. In this way we can use disturbances forecasts to predict the system behavior using \eqref{eq:RTmodel}. This procedure provides a modeling framework that allows to solve optimization problem \eqref{eq:linear_program}. In the next $2$ sections we show how this is achieved. In particular, in the next section we recall the mbCRT algorithm, that uses separation of variables to create a regression trees-based model that can be used to solve optimization problem \eqref{eq:linear_program} for the one step lookahead case, i.e. only with $N=0$. Then we provide a novel methodology, based on the redefinition of the regression trees learning algorithm, so that optimization problem \eqref{eq:RTmodel} can be solved for arbitrary $N$. We finally compare in Sec. \ref{sec:case} the performance of the $2$ approaches to show how the long term prediction can improve system performance.}
