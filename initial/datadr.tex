Our goal is to find data-driven functional models that relates the value of the response variable $\mathbb{Y}$, say power consumption $\mathcal{P}$, with the values of the predictor variables or features $\left[ \mathbb{X}^1,\dots,\mathbb{X}^n \right]$ which can include weather data, set-point information and building schedules.
When the data has lots of features, as is the case in large buildings, which interact in complicated, nonlinear ways, assembling a single global model, such as linear or polynomial regression, can be difficult, and lead to poor response predictions.
An approach to non-linear regression is to partition the data space into smaller regions, where the interactions are more manageable. 
% We then partition the partitions again; this is called recursive partitioning, until finally we get to chunks of the data space which are so tame that we can fit simple models to them. 
%Therefore, the global model has two parts: the recursive partition, and a simple model for each cell of the partition.
Regression trees is an example of an algorithm which belongs to the class of recursive partitioning algorithms. The seminal algorithm for learning regression trees is CART as described in~\cite{breiman1984classification}. 

Regression trees based approaches are our choice of data-driven models for DR-Advisor. The primary reason for this modeling choice is that regression trees are highly interpretable, by design.
Interpretability is a fundamental desirable quality in any predictive model.  
Complex predictive models like neural-networks , support vector regression \etc go through a long calculation routine and involve too many factors. 
It is not easy for a human engineer to judge if the operation/decision is correct or not or how it was generated in the first place. 
Building operators are used to operating a system with fixed logic and rules. 
They tend to prefer models that are more transparent, where it is clear exactly which factors were used to make a particular prediction.
At each node in a regression tree a simple, if this then that, human readable, plain text rule is applied to generate a prediction at the leafs, which anyone can easily understand and interpret.
%Making machine learning algorithms more interpretable is an active area of research~\cite{giraud1998beyond}, one that is essential for incorporating human centric models in cyber-physical energy systems.
Making machine learning algorithms more interpretable is an active area of research, one that is essential for incorporating human centric models in cyber-physical energy systems.

\subsection{Data-Description}

In order to build regression trees which can predict the power consumption of the building, we need to train on time-stamped historical data. As shown in Fig.~\ref{fig:overview}, the data that we use can be divided into four different categories as described below:
\begin{enumerate}
\item \textbf{Weather Data $\mathcal{W}$:} This includes measurements of the outside dry-bulb and wet-bulb air temperature, relative humidity, and wind characteristics. Since we are interested in predicting the power consumption for a finite horizon, we include the weather forecast of the complete horizon in the training features.
\item \textbf{Schedule Data $\mathcal{S}$:} We create proxy variables which correlate with repeated patterns of electricity consumption e.g., due to occupancy or equipment schedules. Day of Week is a categorical predictor which takes values from 1-7 depending on the day of the week. This variable can capture any power consumption patterns which occur on specific days of the week. Likewise, Time of Day is quite an important predictor of power consumption as it can adequately capture daily patterns of occupancy, lighting and appliance use without directly measuring any one of them. Besides using proxy schedule predictors, actual building equipment schedules can also be used as training data for building the trees.
\item \textbf{Building Data $\mathcal{B}$:} The state of the building includes (i) Chilled Water Supply Temperature, (ii) Hot Water Supply Temperature, (iii) Zone Air Temperature, (iv) Supply Air Temperature, and (v) Lighting levels.
\item \textbf{Power Consumption $\mathcal{P}$:} This is the response variable, in addition to zone temperatures. We also considered autoregressive terms of power consumption in the input. 
An auto-regressive tree model is a regular regression tree except that the lagged values of the response variable are also predictor variables for the regression tree.
\end{enumerate}

\subsection{Data-Driven DR Baseline}

DR-Advisor uses a mix of several algorithms to learn a reliable baseline prediction model. For each algorithm, we train the model on historical power consumption data and then validate the predictive capability of the model against a test data-set which the model has never seen before.
In addition to building a single regression tree, we also learn cross-validated regression trees, boosted regression trees (BRT) and random forests (RF). The ensemble methods like BRT and RF help in reducing any over-fitting over the training data. They achieve this by combining the predictions of several base estimators built with a given learning algorithm in order to improve generalizability and robustness over a single estimator.
For a more comprehensive review of random forests we refer the reader to~\cite{breiman2001random} and to~\cite{elith2008working} for an explanation of boosted trees.

\subsection{Data-Driven DR Evaluation}
\label{sec:autort}

The regression tree models for DR evaluation are similar to the models used for DR baseline estimation except for two key differences:
First, instead of only using weather and proxy variables as the training features, in DR evaluation, we also train on set-point schedules and data from the building itself to capture the influence of the state of the building on its power consumption; and 
Second, in order to predict the power consumption of the building for the entire length of the DR event, we use the notion of auto-regressive trees. An auto-regressive tree model is a regular regression tree except that the lagged values of the response variable are also predictor variables for the regression tree \ie the tree structure is learned to approximate the following function:
%\begin{equation}
%\hat{Y_{kW}(t)} = f([X_1, X_2,\cdots, X_m,Y_{kW}(t-1),\cdots,Y_{kW}(t-\delta)])
%\end{equation}
\begin{gather}
  \mathcal{P}(t) = \mathit{f} \left( \mathcal{W}(t), \mathcal{S}(t), \mathcal{B}(t), \mathcal{P}(t-1),\dots, \mathcal{P}(t-\delta) \right).
\label{E:building_model}
\end{gather}
where the predicted power consumption response $\mathcal{P}$ at time $t$, depends on previous values of the response itself $ \mathcal{P}(t-1),\dots, \mathcal{P}(t-\delta)$ and $\delta$ is the order of the auto-regression.
This allows us to make finite horizon predictions of power consumption for the building.
At the beginning of the DR event we use the auto-regressive tree for predicting the response of the building due to each rule-based strategy and choose the one which performs the best over the predicted horizon. The prediction and strategy evaluation is re-computed periodically throughout the event.